{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Exchange Tag Recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data through API call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_list = []\n",
    "\n",
    "def extract_raw_json():\n",
    "    \n",
    "    global json_list\n",
    "    # make requests for 100 pages of size 100 records each \n",
    "    for i in range(1,101):\n",
    "        url = f'https://api.stackexchange.com/2.2/questions?page={i}&pagesize=100&order=desc&sort=activity&site=datascience&key=4*MX83rbQ8xfUvuY*49ZKw(('\n",
    "        data = requests.get(url)\n",
    "        questions_api = data.json()\n",
    "        json_list.append(questions_api)\n",
    "    \n",
    "    #Save the list of extracted records in a json file\n",
    "    with open('stackExchangeAPI.json', 'w+') as f:\n",
    "        json.dump(json_list, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and storing Tags from records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_tags():\n",
    "    #Read each page and record in the file and extract only the tags\n",
    "    with open('stackExchangeAPI.json','r') as json_file:\n",
    "        tags_list = []\n",
    "        data = json.load(json_file)\n",
    "        for i in range(0,99): #Loop through 100 pages\n",
    "            for j in range(0,99): # Loop through 100 records in each page\n",
    "              tags = data[i][\"items\"][j][\"tags\"]\n",
    "              with open('tags.txt','a+') as tags_file: \n",
    "                    tags_file.seek(0)\n",
    "                    d = tags_file.read()\n",
    "                    #If file contains data , add the record in a newline\n",
    "                    if len(d) > 0 :\n",
    "                        tags_file.write(\"\\n\")\n",
    "                    out = ','.join(f'{tags[i]}' for i in range(0,(len(tags))))\n",
    "                    tags_file.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of Unique Tags from tags.txt\n",
    "\n",
    "unique_tags = []\n",
    "\n",
    "def extract_unique_tags():\n",
    "    \n",
    "    global unique_tags\n",
    "    fp  = open('tags.txt')\n",
    "    #Read each line , extract tags split by a comma \n",
    "    tags = [word.strip() for line in fp.readlines() for word in line.split(',') if word.strip()]\n",
    "\n",
    "    print(\" Total number of tags : \" , len(tags))\n",
    "    unique_tags = set(tags)   # Find the unique tags\n",
    "    print(\" Number of Unique tags : \" , len(unique_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Weighted graph of Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty graph with no vertices\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "def create_nodes():\n",
    "    global G\n",
    "    #Create nodes for G \n",
    "    for tag in unique_tags:\n",
    "        G.add_node(tag)\n",
    "    \n",
    "\n",
    "\n",
    "def draw_edges():\n",
    "    global G\n",
    "    related_tags_in_record = []\n",
    "    \n",
    "    fp  = open('tags.txt')\n",
    "    lines = fp.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "            line.strip()\n",
    "            related_tags_in_record.clear()\n",
    "            related_tags_in_record.append(line.split(','))\n",
    "            related_tags_in_record[0] = [e.replace('\\n','')for e in related_tags_in_record[0]]\n",
    "        \n",
    "            #Checking for edges in all possible pairs of tags in a record\n",
    "            for i in range(len(related_tags_in_record[0])):\n",
    "                for j in range(i+1,len(related_tags_in_record[0])):\n",
    "                    if G.has_edge(related_tags_in_record[0][i],related_tags_in_record[0][j]):\n",
    "                    # If edge is already present , increase the edge weight by one\n",
    "                        G[related_tags_in_record[0][i]][related_tags_in_record[0][j]]['weight'] += 1\n",
    "                    else:\n",
    "                    # else , draw a new edge\n",
    "                        G.add_edge(related_tags_in_record[0][i],related_tags_in_record[0][j], weight=1)\n",
    "\n",
    "    edges_list = G.edges.data('weight', default=1)\n",
    "    edges_list = list(edges_list)\n",
    "    print(edges_list[:20])\n",
    "\n",
    "    #Pickling the graph \n",
    "    nx.write_gpickle(G,\"tags_graph.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the neighbouring nodes(connected) of the query \n",
    "\n",
    "def sample_query(query_tag): \n",
    "    edges_of_query = G[query_tag]\n",
    "    \n",
    "    #Sort the edges based on edge weights\n",
    "    neighbours = sorted(edges_of_query.items(), key=lambda edge: edge[1]['weight'],reverse=True)\n",
    "\n",
    "    print(neighbours[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    extract_raw_json()\n",
    "    store_tags()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"tags\": [\n",
      "            \"xgboost\",\n",
      "            \"boosting\",\n",
      "            \"grid-search\"\n",
      "        ],\n",
      "        \"owner\": {\n",
      "            \"reputation\": 261,\n",
      "            \"user_id\": 67931,\n",
      "            \"user_type\": \"registered\",\n",
      "            \"profile_image\": \"https://www.gravatar.com/avatar/dd0f8e431d78371294bbc8aed3cf607f?s=128&d=identicon&r=PG&f=1\",\n",
      "            \"display_name\": \"Maths12\",\n",
      "            \"link\": \"https://datascience.stackexchange.com/users/67931/maths12\"\n",
      "        },\n",
      "        \"is_answered\": false,\n",
      "        \"view_count\": 2,\n",
      "        \"answer_count\": 0,\n",
      "        \"score\": 0,\n",
      "        \"last_activity_date\": 1597336759,\n",
      "        \"creation_date\": 1597336759,\n",
      "        \"question_id\": 80243,\n",
      "        \"content_license\": \"CC BY-SA 4.0\",\n",
      "        \"link\": \"https://datascience.stackexchange.com/questions/80243/why-do-i-need-to-find-update-number-of-boosting-rounds-each-time-i-update-a-para\",\n",
      "        \"title\": \"why do i need to find update number of boosting rounds each time i update a parameter in xgboost?\"\n",
      "    },\n",
      "    {\n",
      "        \"tags\": [\n",
      "            \"dataset\",\n",
      "            \"data\",\n",
      "            \"image-classification\",\n",
      "            \"image-recognition\"\n",
      "        ],\n",
      "        \"owner\": {\n",
      "            \"reputation\": 99,\n",
      "            \"user_id\": 103301,\n",
      "            \"user_type\": \"registered\",\n",
      "            \"profile_image\": \"https://lh4.googleusercontent.com/-7iMWUfNdmUs/AAAAAAAAAAI/AAAAAAAA1hk/tX6vYlQ39Fk/photo.jpg?sz=128\",\n",
      "            \"display_name\": \"user912830823\",\n",
      "            \"link\": \"https://datascience.stackexchange.com/users/103301/user912830823\"\n",
      "        },\n",
      "        \"is_answered\": false,\n",
      "        \"view_count\": 3,\n",
      "        \"answer_count\": 0,\n",
      "        \"score\": -1,\n",
      "        \"last_activity_date\": 1597336134,\n",
      "        \"creation_date\": 1597336134,\n",
      "        \"question_id\": 80242,\n",
      "        \"content_license\": \"CC BY-SA 4.0\",\n",
      "        \"link\": \"https://datascience.stackexchange.com/questions/80242/dataset-for-plant-identification\",\n",
      "        \"title\": \"Dataset for plant identification\"\n",
      "    },\n",
      "    {\n",
      "        \"tags\": [\n",
      "            \"deep-learning\",\n",
      "            \"tensorflow\"\n",
      "        ],\n",
      "        \"owner\": {\n",
      "            \"reputation\": 173,\n",
      "            \"user_id\": 8337,\n",
      "            \"user_type\": \"registered\",\n",
      "            \"profile_image\": \"https://www.gravatar.com/avatar/f1f024ede163fdaebe61807ad5c88f1e?s=128&d=identicon&r=PG&f=1\",\n",
      "            \"display_name\": \"Luca\",\n",
      "            \"link\": \"https://datascience.stackexchange.com/users/8337/luca\"\n",
      "        },\n",
      "        \"is_answered\": true,\n",
      "        \"view_count\": 785,\n",
      "        \"answer_count\": 1,\n",
      "        \"score\": 4,\n",
      "        \"last_activity_date\": 1597335985,\n",
      "        \"creation_date\": 1487532656,\n",
      "        \"question_id\": 17061,\n",
      "        \"content_license\": \"CC BY-SA 3.0\",\n",
      "        \"link\": \"https://datascience.stackexchange.com/questions/17061/combining-trained-neural-nets-in-tensorflow\",\n",
      "        \"title\": \"combining trained neural nets in tensorflow\"\n",
      "    },\n",
      "    {\n",
      "        \"tags\": [\n",
      "            \"neural-network\",\n",
      "            \"deep-learning\",\n",
      "            \"convnet\",\n",
      "            \"cnn\",\n",
      "            \"alex-net\"\n",
      "        ],\n",
      "        \"owner\": {\n",
      "            \"reputation\": 43,\n",
      "            \"user_id\": 51581,\n",
      "            \"user_type\": \"registered\",\n",
      "            \"profile_image\": \"https://lh6.googleusercontent.com/-rOySeCHPEYY/AAAAAAAAAAI/AAAAAAAAEpY/vf_vJGUxQKs/photo.jpg?sz=128\",\n",
      "            \"display_name\": \"Abhishek Saxena\",\n",
      "            \"link\": \"https://datascience.stackexchange.com/users/51581/abhishek-saxena\"\n",
      "        },\n",
      "        \"is_answered\": true,\n",
      "        \"view_count\": 5077,\n",
      "        \"accepted_answer_id\": 31213,\n",
      "        \"answer_count\": 3,\n",
      "        \"score\": 4,\n",
      "        \"last_activity_date\": 1597335982,\n",
      "        \"creation_date\": 1525374473,\n",
      "        \"last_edit_date\": 1597335982,\n",
      "        \"question_id\": 31175,\n",
      "        \"content_license\": \"CC BY-SA 4.0\",\n",
      "        \"link\": \"https://datascience.stackexchange.com/questions/31175/number-of-fully-connected-layers-in-standard-cnns\",\n",
      "        \"title\": \"Number of Fully connected layers in standard CNNs\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json_list[0][\"items\"][:4], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of tags :  123647\n",
      " Number of Unique tags :  589\n"
     ]
    }
   ],
   "source": [
    "extract_unique_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cart', 'ensemble', 'automatic-summarization', 'corpus', 'heatmap', 'weighted-data', 'non-parametric', 'homework', 'genetic-programming', 'data-engineering', 'convergence', 'json', 'hana', 'etl', 'rstudio']\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_tags)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cart', 'ensemble', 'automatic-summarization', 'corpus', 'heatmap', 'weighted-data', 'non-parametric', 'homework', 'genetic-programming', 'data-engineering', 'convergence', 'json', 'hana', 'etl', 'rstudio', 'search', 'one-hot-encoding', 'anomaly-detection', 'alex-net', 'hypothesis-testing']\n"
     ]
    }
   ],
   "source": [
    "create_nodes()\n",
    "\n",
    "nodes = list(G.nodes)\n",
    "\n",
    "print(nodes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cart', 'decision-trees', 13), ('cart', 'boosting', 1), ('cart', 'random-forest', 8), ('cart', 'r', 4), ('cart', 'imbalanced-learn', 4), ('cart', 'machine-learning', 4), ('ensemble', 'classification', 8), ('ensemble', 'prediction', 4), ('ensemble', 'ensemble-modeling', 17), ('ensemble', 'binary', 4), ('ensemble', 'grid-search', 4), ('ensemble', 'ensemble-learning', 2), ('ensemble', 'neural-network', 8), ('ensemble', 'regression', 8), ('ensemble', 'multi-output', 4), ('ensemble', 'machine-learning', 27), ('ensemble', 'python', 8), ('ensemble', 'scikit-learn', 4), ('ensemble', 'xgboost', 4), ('ensemble', 'boosting', 8)]\n"
     ]
    }
   ],
   "source": [
    " draw_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', {'weight': 2186}), ('deep-learning', {'weight': 2160}), ('neural-network', {'weight': 1710}), ('classification', {'weight': 1162}), ('scikit-learn', {'weight': 872}), ('keras', {'weight': 820}), ('regression', {'weight': 612}), ('nlp', {'weight': 610}), ('tensorflow', {'weight': 606}), ('time-series', {'weight': 566})]\n"
     ]
    }
   ],
   "source": [
    "sample_query('machine-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('machine-learning', {'weight': 2707}), ('neural-network', {'weight': 1943}), ('keras', {'weight': 1269}), ('tensorflow', {'weight': 936}), ('python', {'weight': 814}), ('cnn', {'weight': 781}), ('lstm', {'weight': 499}), ('nlp', {'weight': 476}), ('image-classification', {'weight': 406}), ('classification', {'weight': 372})]\n"
     ]
    }
   ],
   "source": [
    "sample_query('deep-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('machine-learning', {'weight': 765}), ('python', {'weight': 532}), ('deep-learning', {'weight': 476}), ('word-embeddings', {'weight': 331}), ('text-mining', {'weight': 285}), ('bert', {'weight': 271}), ('neural-network', {'weight': 235}), ('natural-language-process', {'weight': 225}), ('classification', {'weight': 199}), ('word2vec', {'weight': 198})]\n"
     ]
    }
   ],
   "source": [
    "sample_query('nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"tags_graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
